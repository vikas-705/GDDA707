# -*- coding: utf-8 -*-
"""GDDA707_Assessment 2 V1.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dqP3DHjwXg4lmns3S0RAao3deXF61RLt
"""

import pandas as pd

try:
    supply_chain_df = pd.read_csv('DataCoSupplyChainDataset.csv', encoding='ISO-8859-1')
except UnicodeDecodeError:
    print("Encoding error: Trying a different encoding.")
    # Try another encoding if needed
    supply_chain_df = pd.read_csv('DataCoSupplyChainDataset.csv', encoding='cp1252')

# Repeat for the second dataset
try:
    credit_df = pd.read_csv('dataset.csv', encoding='ISO-8859-1')
except UnicodeDecodeError:
    print("Encoding error: Trying a different encoding.")
    # Try another encoding if needed
    credit_df = pd.read_csv('dataset.csv', encoding='cp1252')

print(supply_chain_df.info())
print(supply_chain_df.head())

supply_chain_df = supply_chain_df.dropna(axis=1, thresh=len(supply_chain_df) * 0.5)

for col in supply_chain_df.select_dtypes(include=['float64', 'int64']).columns:
    supply_chain_df[col].fillna(supply_chain_df[col].median(), inplace=True)

for col in supply_chain_df.select_dtypes(include=['object']).columns:
    supply_chain_df[col].fillna(supply_chain_df[col].mode()[0], inplace=True)

supply_chain_df['order date (DateOrders)'] = pd.to_datetime(supply_chain_df['order date (DateOrders)'])
supply_chain_df['shipping date (DateOrders)'] = pd.to_datetime(supply_chain_df['shipping date (DateOrders)'])

credit_df = credit_df[credit_df['Credit amount'] >= 0]  # Example filter

print(credit_df.info())
print(credit_df.head())

credit_df = credit_df.dropna(axis=1, thresh=len(credit_df) * 0.5)

for col in credit_df.select_dtypes(include=['float64', 'int64']).columns:
    credit_df[col].fillna(credit_df[col].median(), inplace=True)

for col in credit_df.select_dtypes(include=['object']).columns:
    credit_df[col].fillna(credit_df[col].mode()[0], inplace=True)

credit_df = credit_df[credit_df['Credit amount'] >= 0]

from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for col in supply_chain_df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    supply_chain_df[col] = le.fit_transform(supply_chain_df[col].astype(str))
    label_encoders[col] = le

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
numeric_cols = supply_chain_df.select_dtypes(include=['float64', 'int64']).columns
supply_chain_df[numeric_cols] = scaler.fit_transform(supply_chain_df[numeric_cols])

label_encoders = {}
for col in credit_df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    credit_df[col] = le.fit_transform(credit_df[col].astype(str))
    label_encoders[col] = le

scaler = MinMaxScaler()
numeric_cols = credit_df.select_dtypes(include=['float64', 'int64']).columns
credit_df[numeric_cols] = scaler.fit_transform(credit_df[numeric_cols])

supply_chain_df.to_csv('cleaned_DataCoSupplyChainDataset.csv', index=False)
credit_df.to_csv('cleaned_dataset.csv', index=False)

credit_df.to_csv

supply_chain_df.to_csv

print(supply_chain_df.info())

supply_chain_df['order date (DateOrders)'] = pd.to_datetime(supply_chain_df['order date (DateOrders)'], errors='coerce')
supply_chain_df['shipping date (DateOrders)'] = pd.to_datetime(supply_chain_df['shipping date (DateOrders)'], errors='coerce')

supply_chain_df.fillna({'Product Description': 'Unknown'}, inplace=True)

print(credit_df.info())

credit_df['Credit history'] = credit_df['Credit history'].astype('category')
credit_df['Purpose of the credit'] = credit_df['Purpose of the credit'].astype('category')
credit_df['Status of savings account/bonds'] = credit_df['Status of savings account/bonds'].astype('category')

# Temporarily convert categorical columns to string
if 'Status of savings account/bonds' in credit_df.columns:
    if credit_df['Status of savings account/bonds'].dtype.name == 'category':
        credit_df['Status of savings account/bonds'] = credit_df['Status of savings account/bonds'].astype(str)
        credit_df['Status of savings account/bonds'].fillna('Unknown', inplace=True)
        # Convert back to categorical if needed
        credit_df['Status of savings account/bonds'] = credit_df['Status of savings account/bonds'].astype('category')

print("Data transformation complete.")

unified_df = pd.concat([supply_chain_df, credit_df], axis=0, ignore_index=True)

unified_df.drop_duplicates(inplace=True)

unified_df.reset_index(drop=True, inplace=True)

print("Data loaded and integrated successfully.")
print(unified_df.head())

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder \
    .appName("Data Ingestion Pipeline for Supply Chain Data") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .getOrCreate()

source_path = "path/to/DataCoSupplyChainDataset.csv"

# Replace 'namenode' with the actual hostname or IP address of your Hadoop NameNode
spark = SparkSession.builder \
    .appName("Data Ingestion Pipeline for Supply Chain Data") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://<namenode-IP>:9000") \
    .getOrCreate()

source_path = "path/to/DataCoSupplyChainDataset.csv"

spark = SparkSession.builder \
    .appName("Data Ingestion Pipeline for Supply Chain Data") \
    .getOrCreate()

supply_chain_df = supply_chain_df.fillna({
    "Order ID": "Unknown",
    "Order Date": "Unknown",
    "Order Status": "Unknown"
})

local_path = "path/to/processed_supply_chain_data"


print("Data ingestion pipeline completed successfully.")

# Stop Spark session
spark.stop()

pip install pymongo

from pymongo import MongoClient

client = MongoClient('mongodb://localhost:27017/')

db = client['supply_chain_db']

collection = db['supply_chain_data']

data = [
    {"OrderID": "1001", "OrderDate": "2024-07-01", "OrderStatus": "Shipped", "Amount": 150.0},
    {"OrderID": "1002", "OrderDate": "2024-07-02", "OrderStatus": "Pending", "Amount": 200.0},
    {"OrderID": "1003", "OrderDate": "2024-07-03", "OrderStatus": "Delivered", "Amount": 250.0}
]

def query_data():
    # Find all documents
    results = collection.find()
    for result in results:
        print(result)

def query_by_status(status):
    # Find documents by status
    results = collection.find({"OrderStatus": status})
    for result in results:
        print(result)

print("All data:")
query_data()

# Query by status
print("\nOrders with status 'Shipped':")
query_by_status('Shipped')

# Close the MongoDB connection
client.close()

pip install tweepy

import tweepy

consumer_key = 'your_consumer_key'
consumer_secret = 'your_consumer_secret'
access_token = 'your_access_token'
access_token_secret = 'your_access_token_secret'

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.streaming import StreamingContext

spark = SparkSession.builder \
    .appName("SocialMediaClustering") \
    .getOrCreate()

ssc = StreamingContext(spark.sparkContext, 10)

pip install pyspark confluent-kafka

spark.stop()

pip install confluent-kafka

from confluent_kafka import Producer
import json

conf = {'bootstrap.servers': 'localhost:9092'}

producer = Producer(conf)

def delivery_report(err, msg):
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))

for i in range(100):
    data = {'message': f'Test message {i}'}
    producer.produce('my_topic', key=str(i), value=json.dumps(data), callback=delivery_report)
    producer.poll(0)

producer.flush()